# Sensor Data Ingest

Scripts in this repository do all the work of converting raw data from Purple
Air sensors into .rda files ready for use with the **AirSensor** R package.

Data can be accesed in R with:

```
library(AirSensor)
setArchiveBaseUrl("https://airfire-data-exports.s3-us-west-2.amazonaws.com/PurpleAir/v1")
```

## Installation Instructions for an Operational Site

### Docker containers

_For background on Docker, see:_

* https://en.wikipedia.org/wiki/Docker_(software)
* https://www.docker.com

All data proceessing is performed by scripts running inside of docker
containers. This level of virtualization allows containers and scripts to be
loaded onto a system that has none of the other software dependencies required
to run R.

The package source code incluces a `docker/Makefile` with targets and 
dependencies to simplify building a docker image.

The docker image with the tag `mazamascience/airsensor:latest` must exist on
the host machine in order to run the data ingest scripts described below.

You can review current `airsensor` docker images with:

```
docker images | grep airsensor
```

### Web accessible directories

It is assumed that scripts are being run on a Unix system with an Apache
web server. A data directory should be set up as the `archiveBaseDir` so that
Apache can serve GET requests for data files.

An example base directory might be:

/data/PurpleAir/

### Cron jobs

Each of the `~_exec.R` scripts is run on a daily schedule defined by
`crontab_daily.txt`.

_Note that all crontab entries must be on a single line. No line continuation
characters are allowed._

The `crontab` files, along with `test/Makefile` use the docker `-v` flag to 
mount host directories (aka "volumes") to predefined locations inside the
docker container.

To deploy the data ingest scripts, the contents of the `crontab` files should be 
modified to reflect appropriate absolute paths on the host machine and then
added to a privileged user's crontab so the scripts will be run on a daily basis.

## Details

### Files

This directory hasthe following contents:

```
├── README.md
├── createAirSensor_extended_exec.R
├── createAirSensor_latest_exec.R
├── createPAS_exec.R
├── createPAT_extended_exec.R
├── createPAT_latest_exec.R
├── crontab_archive.txt
├── crontab_daily.txt
├── OLD_createMonthlyAirSensor_exec.R
├── OLD_createMonthlyPAT_exec.R
├── OLD_createPAS_archival_exec.R
└── test
    └── Makefile
```

Each of the `~_exec.R` scripts is run on a daily schedule defined by
`crontab_daily.txt`.

### Output Directories

As each script is run, either at the command line or from a cron job, it will 
generate output files in the directory specified with the `--outputDir` option.
The following directory structure is required. R package functions assume the 
following directory structure will be available  at some web accessible  
`archiveBaseDir` or `archiveBasUrl`:

```
├── airsensor
│   ├── 2018
│   ├── 2019
│   ├── 2020
│   └── latest
├── pas
│   ├── 2020
│   └── 2019
├── pat
│   ├── 2018
│   ├── 2019
│   ├── 2020
│   └── latest
└── videos
    ├── 2018
    └── 2019
```

Files generated by the `Latest` scripts are always written into `latest/`
directories while other scripts write datestamped files into the appropriate
annual directory.

### Processing Logs

As each script is run, either at the command line or from a cron job, it will 
generate logging output in the directory specified with the `--logDir` option. 
Log files contain the name of the processing script. Four different levels 
of logging are provided:

 * `ERROR` -- Something went wrong, sometimes resulting in no generation of an output file.
 * `INFO` -- Summary information on data processed along with any warnings generated.
 * `DEBUG` -- Detailed processing information to help understand where processing might have gone wrong.
 * `TRACE` -- *Excruciatingly* detailed processing information including URL requests.

Note that scripts run repeatedly in cron jobs will overwrite the logs so that 
any failures seen in the log files represent the most recent run of the script
generating the failure.

### Testing

The `test/` directory contains a `Makefile` with targets that will run
executable scripts using `mazamascience/airsensor:latest` docker image. All
output and log files will be generated in `output/` and `logs/` directories
that can be removed with the `clean` target.

For example, to test the proper generation of `pas_archival` files, one should:

```
cd test; make createPAS_archival
```

Then review the files generated in `output/` and `logs/`.

More detailed debugging can be performed by loading the `~_exec.R` scripts into
RStudio and running them interactively.

### Copying files to S3

These scripts will typically be deployed on an AWS virtual machine. The AirFire
group uses AWS S3 buckets for archival data so files need to be copied to the
appropriate location after they are created.

This is handled by publishing code found in the **pnwairfire-data-scripts**
repository. Read more in that repo. The specific configuration script for
the Purple Air data is `config/purple-air-production.json`.

You have to build a docker image with that publication information and then
edit the crontab on the VM so that the publisher is run at regular intervals.

### Background Reading

A quick refresher on docker commands is available at the 
[docker cheat sheet](https://github.com/wsargent/docker-cheat-sheet).

